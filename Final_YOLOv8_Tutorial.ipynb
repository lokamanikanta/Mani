{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bf7a65-bc82-46bc-ec5c-6aaaf24e961a"
      },
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 29.0/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Predict\n",
        "\n",
        "YOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLOv8 Predict Docs](https://docs.ultralytics.com/modes/train/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4906fe10-417e-475f-cc01-7d34e140f0d2"
      },
      "source": [
        "# Run inference on an image with YOLOv8n\n",
        "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 77.5MB/s]\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "Downloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg'...\n",
            "100% 165k/165k [00:00<00:00, 7.25MB/s]\n",
            "image 1/1 /content/zidane.jpg: 384x640 2 persons, 1 tie, 76.4ms\n",
            "Speed: 13.6ms preprocess, 76.4ms inference, 4346.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/212889447-69e5bdf1-5800-4e29-835e-2ed2336dede2.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Val\n",
        "Validate a model's accuracy on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset's `val` or `test` splits. The latest YOLOv8 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLOv8 Val Docs](https://docs.ultralytics.com/modes/val/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQPtK1QYVaD_",
        "outputId": "56c6accd-3a9f-4147-b956-b6217bf694b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download COCO val\n",
        "import torch\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780M/780M [00:11<00:00, 72.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH",
        "outputId": "859fddc1-fc49-4f2a-c261-b88435a16e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Validate YOLOv8n on COCO8 val\n",
        "!yolo val model=yolov8n.pt data=coco8.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "Dataset 'coco8.yaml' images not found âš ï¸, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n",
            "100% 433k/433k [00:00<00:00, 12.6MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100% 25/25 [00:00<00:00, 3936.24file/s]\n",
            "Dataset download success âœ… (0.8s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 17.1MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<00:00, 173.93it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.49s/it]\n",
            "                   all          4         17      0.621      0.833      0.888       0.63\n",
            "                person          4         10      0.721        0.5      0.519      0.269\n",
            "                   dog          4          1       0.37          1      0.995      0.597\n",
            "                 horse          4          2      0.751          1      0.995      0.631\n",
            "              elephant          4          2      0.505        0.5      0.828      0.394\n",
            "              umbrella          4          1      0.564          1      0.995      0.995\n",
            "          potted plant          4          1      0.814          1      0.995      0.895\n",
            "Speed: 0.8ms preprocess, 30.3ms inference, 0.0ms loss, 448.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "outputId": "d7a0adf1-4543-4e87-d00f-a4c73ba55946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train YOLOv8n on COCO8 for 3 epochs\n",
        "!yolo train model=yolov8n.pt data=coco8.yaml epochs=3 imgsz=640"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "2024-04-16 13:52:17.128814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-16 13:52:17.128876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-16 13:52:17.270637: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n",
            "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<00:00, 43351.98it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3      0.81G      1.039      3.146      1.498         25        640: 100% 1/1 [00:01<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.26it/s]\n",
            "                   all          4         17       0.62      0.885      0.888      0.621\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3     0.822G      1.169      2.779      1.442         36        640: 100% 1/1 [00:00<00:00,  7.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.31it/s]\n",
            "                   all          4         17      0.595      0.903      0.888      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3      0.82G     0.6701      3.697      1.096         17        640: 100% 1/1 [00:00<00:00,  7.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  5.66it/s]\n",
            "                   all          4         17      0.577      0.833      0.874      0.614\n",
            "\n",
            "3 epochs completed in 0.002 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00, 22.89it/s]\n",
            "                   all          4         17      0.617      0.884      0.888      0.622\n",
            "                person          4         10       0.67        0.5       0.52      0.278\n",
            "                   dog          4          1      0.361          1      0.995      0.597\n",
            "                 horse          4          2      0.728          1      0.995      0.631\n",
            "              elephant          4          2      0.602      0.805      0.828      0.332\n",
            "              umbrella          4          1      0.553          1      0.995      0.995\n",
            "          potted plant          4          1      0.789          1      0.995      0.895\n",
            "Speed: 0.2ms preprocess, 3.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- ðŸ’¡ ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.  \n",
        "- ðŸ’¡ ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format                                                             | `format` Argument | Model                     | Metadata | Arguments                                           |\n",
        "|--------------------------------------------------------------------|-------------------|---------------------------|----------|-----------------------------------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | âœ…        | -                                                   |\n",
        "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | âœ…        | `imgsz`, `optimize`                                 |\n",
        "| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |\n",
        "| [OpenVINO](https://docs.openvino.ai/)                              | `openvino`        | `yolov8n_openvino_model/` | âœ…        | `imgsz`, `half`, `int8`                             |\n",
        "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |\n",
        "| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlpackage`       | âœ…        | `imgsz`, `half`, `int8`, `nms`                      |\n",
        "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | âœ…        | `imgsz`, `keras`, `int8`                            |\n",
        "| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | âŒ        | `imgsz`                                             |\n",
        "| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | âœ…        | `imgsz`, `half`, `int8`                             |\n",
        "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | âœ…        | `imgsz`                                             |\n",
        "| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | âœ…        | `imgsz`, `half`, `int8`                             |\n",
        "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | âœ…        | `imgsz`                                             |\n",
        "| [NCNN](https://github.com/Tencent/ncnn)                            | `ncnn`            | `yolov8n_ncnn_model/`     | âœ…        | `imgsz`, `half`                                     |\n"
      ],
      "metadata": {
        "id": "nPZZeNrLCQG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=yolov8n.pt format=torchscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIjW4igCjqD",
        "outputId": "b3ae693a-2862-4358-c221-1b929a3a0e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.30GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.2.1+cu121...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 4.8s, saved as 'yolov8n.torchscript' (12.4 MB)\n",
            "\n",
            "Export complete (7.0s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLOv8 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLOv8 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLOv8 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ],
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data='coco128.yaml', epochs=3)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set\n",
        "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
        "results = model.export(format='onnx')  # export the model to ONNX format"
      ],
      "metadata": {
        "id": "bpF9-vS_DAaf",
        "outputId": "225559e7-7b56-40f8-e8ff-eda96f1c3443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "\n",
            "Dataset 'coco128.yaml' images not found âš ï¸, missing path '/content/datasets/coco128/images/train2017'\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to '/content/datasets/coco128.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.66M/6.66M [00:00<00:00, 76.6MB/s]\n",
            "Unzipping /content/datasets/coco128.zip to /content/datasets/coco128...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 263/263 [00:00<00:00, 351.76file/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success âœ… (1.8s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 1745.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco128/labels/train2017.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/3      2.71G      1.146      1.453      1.247        130        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.631      0.541      0.615      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/3      2.77G      1.215       1.45      1.274        178        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.661      0.546      0.628      0.467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/3      2.59G      1.157      1.342      1.232        280        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  2.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.655      0.554      0.632      0.469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3 epochs completed in 0.008 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.653      0.555      0.632      0.468\n",
            "                person        128        254      0.792      0.673       0.77      0.542\n",
            "               bicycle        128          6      0.491      0.333      0.329      0.282\n",
            "                   car        128         46        0.9      0.217       0.28      0.187\n",
            "            motorcycle        128          5      0.692      0.905      0.938      0.743\n",
            "              airplane        128          6      0.825      0.792      0.927      0.717\n",
            "                   bus        128          7      0.679      0.714      0.722      0.661\n",
            "                 train        128          3      0.545      0.667      0.741      0.677\n",
            "                 truck        128         12          1      0.403        0.5      0.319\n",
            "                  boat        128          6      0.242      0.167      0.308      0.171\n",
            "         traffic light        128         14      0.736      0.202      0.202      0.139\n",
            "             stop sign        128          2       0.89          1      0.995      0.716\n",
            "                 bench        128          9      0.826      0.667      0.639      0.391\n",
            "                  bird        128         16      0.925      0.777      0.896      0.543\n",
            "                   cat        128          4       0.86          1      0.995      0.799\n",
            "                   dog        128          9      0.652      0.834      0.841      0.598\n",
            "                 horse        128          2      0.549          1      0.995      0.517\n",
            "              elephant        128         17      0.781      0.765      0.885      0.661\n",
            "                  bear        128          1      0.614          1      0.995      0.995\n",
            "                 zebra        128          4      0.854          1      0.995      0.965\n",
            "               giraffe        128          9      0.863          1      0.962      0.724\n",
            "              backpack        128          6      0.615      0.333      0.377      0.237\n",
            "              umbrella        128         18      0.636      0.444      0.639      0.446\n",
            "               handbag        128         19      0.646     0.0526      0.213      0.121\n",
            "                   tie        128          7      0.823      0.714      0.676      0.463\n",
            "              suitcase        128          4       0.63          1      0.828        0.6\n",
            "               frisbee        128          5      0.578        0.8       0.76      0.664\n",
            "                  skis        128          1      0.472          1      0.995      0.497\n",
            "             snowboard        128          7      0.705      0.714      0.755      0.486\n",
            "           sports ball        128          6      0.718      0.437      0.515       0.28\n",
            "                  kite        128         10      0.833        0.5      0.577      0.195\n",
            "          baseball bat        128          4      0.427       0.39      0.378      0.162\n",
            "        baseball glove        128          7      0.649      0.429      0.429      0.295\n",
            "            skateboard        128          5      0.769        0.6        0.6      0.427\n",
            "         tennis racket        128          7      0.506      0.294      0.467      0.311\n",
            "                bottle        128         18      0.465      0.444      0.378      0.207\n",
            "            wine glass        128         16      0.663        0.5      0.625      0.359\n",
            "                   cup        128         36      0.612      0.263      0.405      0.284\n",
            "                  fork        128          6      0.581      0.167      0.265      0.194\n",
            "                 knife        128         16      0.686      0.546      0.618      0.373\n",
            "                 spoon        128         22      0.533      0.208      0.322       0.19\n",
            "                  bowl        128         28      0.662      0.679      0.634      0.527\n",
            "                banana        128          1          0          0      0.166     0.0451\n",
            "              sandwich        128          2      0.411      0.732      0.497      0.497\n",
            "                orange        128          4          1       0.35      0.995      0.666\n",
            "              broccoli        128         11      0.438      0.182      0.253      0.222\n",
            "                carrot        128         24      0.708      0.506      0.687      0.443\n",
            "               hot dog        128          2      0.637      0.912      0.828      0.796\n",
            "                 pizza        128          5      0.699          1      0.995       0.86\n",
            "                 donut        128         14      0.647          1      0.934      0.845\n",
            "                  cake        128          4      0.645          1      0.995       0.88\n",
            "                 chair        128         35      0.496      0.514      0.453      0.267\n",
            "                 couch        128          6      0.404        0.5      0.611      0.427\n",
            "          potted plant        128         14      0.657      0.643      0.731      0.485\n",
            "                   bed        128          3      0.722      0.667      0.863      0.705\n",
            "          dining table        128         13      0.472      0.615      0.498      0.398\n",
            "                toilet        128          2      0.623        0.5      0.828      0.763\n",
            "                    tv        128          2      0.605      0.816      0.828      0.762\n",
            "                laptop        128          3          1          0      0.736      0.582\n",
            "                 mouse        128          2          1          0     0.0458    0.00458\n",
            "                remote        128          8      0.847        0.5      0.578      0.495\n",
            "            cell phone        128          8          0          0     0.0642     0.0413\n",
            "             microwave        128          3      0.515      0.667       0.83      0.702\n",
            "                  oven        128          5      0.414        0.4      0.343      0.273\n",
            "                  sink        128          6      0.355      0.167      0.249      0.157\n",
            "          refrigerator        128          5      0.568        0.4      0.644      0.518\n",
            "                  book        128         29      0.635      0.121      0.383      0.198\n",
            "                 clock        128          9      0.779      0.784      0.894      0.741\n",
            "                  vase        128          2      0.388          1      0.828      0.795\n",
            "              scissors        128          1          1          0      0.249     0.0746\n",
            "            teddy bear        128         21          1      0.371      0.622       0.42\n",
            "            toothbrush        128          5      0.755        0.4       0.84      0.527\n",
            "Speed: 0.6ms preprocess, 4.4ms inference, 0.0ms loss, 8.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.656      0.559       0.63      0.466\n",
            "                person        128        254      0.794      0.681      0.777      0.546\n",
            "               bicycle        128          6       0.49      0.333      0.326      0.271\n",
            "                   car        128         46      0.847      0.217      0.278      0.187\n",
            "            motorcycle        128          5      0.692      0.907      0.938      0.742\n",
            "              airplane        128          6      0.825      0.793      0.927      0.717\n",
            "                   bus        128          7      0.631      0.714      0.722       0.66\n",
            "                 train        128          3      0.544      0.667      0.747      0.682\n",
            "                 truck        128         12      0.966      0.417      0.489      0.282\n",
            "                  boat        128          6      0.226      0.167      0.354      0.149\n",
            "         traffic light        128         14      0.723      0.191      0.202      0.139\n",
            "             stop sign        128          2      0.884          1      0.995      0.708\n",
            "                 bench        128          9      0.824      0.667      0.639      0.391\n",
            "                  bird        128         16      0.926      0.778      0.893      0.531\n",
            "                   cat        128          4      0.858          1      0.995      0.799\n",
            "                   dog        128          9      0.664       0.88      0.841      0.598\n",
            "                 horse        128          2      0.545          1      0.995      0.517\n",
            "              elephant        128         17      0.828      0.765      0.891      0.661\n",
            "                  bear        128          1      0.614          1      0.995      0.995\n",
            "                 zebra        128          4      0.853          1      0.995      0.965\n",
            "               giraffe        128          9      0.802          1      0.935      0.729\n",
            "              backpack        128          6      0.627      0.333      0.392      0.243\n",
            "              umbrella        128         18      0.616      0.445      0.633      0.442\n",
            "               handbag        128         19      0.682     0.0526      0.206      0.119\n",
            "                   tie        128          7      0.823      0.714      0.676      0.463\n",
            "              suitcase        128          4      0.626          1      0.828        0.6\n",
            "               frisbee        128          5      0.576        0.8       0.76      0.664\n",
            "                  skis        128          1      0.458          1      0.995      0.497\n",
            "             snowboard        128          7      0.705      0.714      0.753      0.485\n",
            "           sports ball        128          6      0.718      0.436      0.502      0.261\n",
            "                  kite        128         10      0.827        0.5      0.579       0.21\n",
            "          baseball bat        128          4      0.393       0.25      0.354        0.2\n",
            "        baseball glove        128          7      0.628      0.429      0.429      0.295\n",
            "            skateboard        128          5      0.828        0.6        0.6       0.44\n",
            "         tennis racket        128          7      0.742      0.415      0.487       0.33\n",
            "                bottle        128         18      0.433      0.425      0.376      0.213\n",
            "            wine glass        128         16      0.615        0.5      0.604      0.345\n",
            "                   cup        128         36      0.605      0.299      0.421      0.296\n",
            "                  fork        128          6      0.573      0.167      0.228       0.18\n",
            "                 knife        128         16      0.493        0.5      0.586      0.348\n",
            "                 spoon        128         22      0.603      0.182      0.339      0.189\n",
            "                  bowl        128         28      0.685        0.7      0.665      0.511\n",
            "                banana        128          1          0          0      0.111     0.0336\n",
            "              sandwich        128          2      0.302        0.5      0.398      0.398\n",
            "                orange        128          4          1      0.352      0.995      0.666\n",
            "              broccoli        128         11      0.435      0.182      0.259      0.208\n",
            "                carrot        128         24      0.736       0.58      0.686      0.428\n",
            "               hot dog        128          2      0.632      0.896      0.828      0.796\n",
            "                 pizza        128          5      0.789          1      0.995      0.843\n",
            "                 donut        128         14      0.647          1      0.933      0.847\n",
            "                  cake        128          4      0.631          1      0.945      0.845\n",
            "                 chair        128         35      0.501      0.543      0.459      0.259\n",
            "                 couch        128          6      0.504        0.5      0.716      0.531\n",
            "          potted plant        128         14      0.654      0.643      0.729      0.498\n",
            "                   bed        128          3          1      0.949      0.995      0.742\n",
            "          dining table        128         13      0.446      0.538      0.493      0.395\n",
            "                toilet        128          2      0.622        0.5      0.828      0.796\n",
            "                    tv        128          2      0.607      0.821      0.828      0.762\n",
            "                laptop        128          3          1          0      0.706      0.567\n",
            "                 mouse        128          2          1          0     0.0638    0.00638\n",
            "                remote        128          8      0.846        0.5      0.605       0.51\n",
            "            cell phone        128          8          0          0     0.0651     0.0417\n",
            "             microwave        128          3      0.514      0.667       0.83      0.717\n",
            "                  oven        128          5      0.421        0.4      0.344      0.273\n",
            "                  sink        128          6      0.372      0.167      0.201      0.134\n",
            "          refrigerator        128          5      0.617        0.4      0.628      0.496\n",
            "                  book        128         29      0.649      0.129       0.37      0.202\n",
            "                 clock        128          9      0.779      0.786      0.896      0.738\n",
            "                  vase        128          2      0.389          1      0.828      0.795\n",
            "              scissors        128          1          1          0      0.249     0.0746\n",
            "            teddy bear        128         21          1      0.372      0.624      0.421\n",
            "            toothbrush        128          5      0.685        0.6      0.803      0.492\n",
            "Speed: 0.4ms preprocess, 15.8ms inference, 0.0ms loss, 9.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train22\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 476k/476k [00:00<00:00, 10.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 219.0ms\n",
            "Speed: 7.9ms preprocess, 219.0ms inference, 55.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.30GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/detect/train2/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15.9/15.9 MB 39.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.16.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 10.2s, installed 1 package: ['onnx>=1.12.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 11.1s, saved as 'runs/detect/train2/weights/best.onnx' (12.2 MB)\n",
            "\n",
            "Export complete (13.0s)\n",
            "Results saved to \u001b[1m/content/runs/detect/train2/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=runs/detect/train2/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=runs/detect/train2/weights/best.onnx imgsz=640 data=/usr/local/lib/python3.10/dist-packages/ultralytics/cfg/datasets/coco128.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Tasks\n",
        "\n",
        "YOLOv8 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLOv8 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n",
        "\n",
        "<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\">\n"
      ],
      "metadata": {
        "id": "Phm9ccmOKye5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Detection\n",
        "\n",
        "YOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n"
      ],
      "metadata": {
        "id": "yq26lwpYK1lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
        "model.train(data='coco128.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "8Go5qqS9LbC5",
        "outputId": "377d5f20-1e93-49ed-e95c-e5bc8986eb06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/3      2.77G      1.146      1.453      1.247        130        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:11<00:00,  1.50s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.631      0.541      0.615      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/3      2.85G      1.215       1.45      1.274        178        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.661      0.546      0.628      0.467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/3      2.58G      1.157      1.342      1.232        280        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.655      0.554      0.632      0.469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3 epochs completed in 0.019 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.653      0.555      0.632      0.468\n",
            "                person        128        254      0.792      0.673       0.77      0.542\n",
            "               bicycle        128          6      0.491      0.333      0.329      0.282\n",
            "                   car        128         46        0.9      0.217       0.28      0.187\n",
            "            motorcycle        128          5      0.692      0.905      0.938      0.743\n",
            "              airplane        128          6      0.825      0.792      0.927      0.717\n",
            "                   bus        128          7      0.679      0.714      0.722      0.661\n",
            "                 train        128          3      0.545      0.667      0.741      0.677\n",
            "                 truck        128         12          1      0.403        0.5      0.319\n",
            "                  boat        128          6      0.242      0.167      0.308      0.171\n",
            "         traffic light        128         14      0.736      0.202      0.202      0.139\n",
            "             stop sign        128          2       0.89          1      0.995      0.716\n",
            "                 bench        128          9      0.826      0.667      0.639      0.391\n",
            "                  bird        128         16      0.925      0.777      0.896      0.543\n",
            "                   cat        128          4       0.86          1      0.995      0.799\n",
            "                   dog        128          9      0.652      0.834      0.841      0.598\n",
            "                 horse        128          2      0.549          1      0.995      0.517\n",
            "              elephant        128         17      0.781      0.765      0.885      0.661\n",
            "                  bear        128          1      0.614          1      0.995      0.995\n",
            "                 zebra        128          4      0.854          1      0.995      0.965\n",
            "               giraffe        128          9      0.863          1      0.962      0.724\n",
            "              backpack        128          6      0.615      0.333      0.377      0.237\n",
            "              umbrella        128         18      0.636      0.444      0.639      0.446\n",
            "               handbag        128         19      0.646     0.0526      0.213      0.121\n",
            "                   tie        128          7      0.823      0.714      0.676      0.463\n",
            "              suitcase        128          4       0.63          1      0.828        0.6\n",
            "               frisbee        128          5      0.578        0.8       0.76      0.664\n",
            "                  skis        128          1      0.472          1      0.995      0.497\n",
            "             snowboard        128          7      0.705      0.714      0.755      0.486\n",
            "           sports ball        128          6      0.718      0.437      0.515       0.28\n",
            "                  kite        128         10      0.833        0.5      0.577      0.195\n",
            "          baseball bat        128          4      0.427       0.39      0.378      0.162\n",
            "        baseball glove        128          7      0.649      0.429      0.429      0.295\n",
            "            skateboard        128          5      0.769        0.6        0.6      0.427\n",
            "         tennis racket        128          7      0.506      0.294      0.467      0.311\n",
            "                bottle        128         18      0.465      0.444      0.378      0.207\n",
            "            wine glass        128         16      0.663        0.5      0.625      0.359\n",
            "                   cup        128         36      0.612      0.263      0.405      0.284\n",
            "                  fork        128          6      0.581      0.167      0.265      0.194\n",
            "                 knife        128         16      0.686      0.546      0.618      0.373\n",
            "                 spoon        128         22      0.533      0.208      0.322       0.19\n",
            "                  bowl        128         28      0.662      0.679      0.634      0.527\n",
            "                banana        128          1          0          0      0.166     0.0451\n",
            "              sandwich        128          2      0.411      0.732      0.497      0.497\n",
            "                orange        128          4          1       0.35      0.995      0.666\n",
            "              broccoli        128         11      0.438      0.182      0.253      0.222\n",
            "                carrot        128         24      0.708      0.506      0.687      0.443\n",
            "               hot dog        128          2      0.637      0.912      0.828      0.796\n",
            "                 pizza        128          5      0.699          1      0.995       0.86\n",
            "                 donut        128         14      0.647          1      0.934      0.845\n",
            "                  cake        128          4      0.645          1      0.995       0.88\n",
            "                 chair        128         35      0.496      0.514      0.453      0.267\n",
            "                 couch        128          6      0.404        0.5      0.611      0.427\n",
            "          potted plant        128         14      0.657      0.643      0.731      0.485\n",
            "                   bed        128          3      0.722      0.667      0.863      0.705\n",
            "          dining table        128         13      0.472      0.615      0.498      0.398\n",
            "                toilet        128          2      0.623        0.5      0.828      0.763\n",
            "                    tv        128          2      0.605      0.816      0.828      0.762\n",
            "                laptop        128          3          1          0      0.736      0.582\n",
            "                 mouse        128          2          1          0     0.0458    0.00458\n",
            "                remote        128          8      0.847        0.5      0.578      0.495\n",
            "            cell phone        128          8          0          0     0.0642     0.0413\n",
            "             microwave        128          3      0.515      0.667       0.83      0.702\n",
            "                  oven        128          5      0.414        0.4      0.343      0.273\n",
            "                  sink        128          6      0.355      0.167      0.249      0.157\n",
            "          refrigerator        128          5      0.568        0.4      0.644      0.518\n",
            "                  book        128         29      0.635      0.121      0.383      0.198\n",
            "                 clock        128          9      0.779      0.784      0.894      0.741\n",
            "                  vase        128          2      0.388          1      0.828      0.795\n",
            "              scissors        128          1          1          0      0.249     0.0746\n",
            "            teddy bear        128         21          1      0.371      0.622       0.42\n",
            "            toothbrush        128          5      0.755        0.4       0.84      0.527\n",
            "Speed: 0.4ms preprocess, 3.8ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 20.4ms\n",
            "Speed: 3.4ms preprocess, 20.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " masks: None\n",
              " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
              " obb: None\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: None\n",
              " save_dir: 'runs/detect/train32'\n",
              " speed: {'preprocess': 3.4084320068359375, 'inference': 20.361661911010742, 'postprocess': 2.216815948486328}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "YOLOv8 _segmentation_ models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n"
      ],
      "metadata": {
        "id": "7ZW58jUzK66B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
        "model.train(data='coco128-seg.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "WFPJIQl_L5HT",
        "outputId": "31c9bbf8-f45c-46db-fbb4-2d68efef6f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-seg.pt to 'yolov8n-seg.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.73M/6.73M [00:00<00:00, 86.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=coco128-seg.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train\n",
            "\n",
            "Dataset 'coco128-seg.yaml' images not found âš ï¸, missing path '/content/datasets/coco128-seg/images/train2017'\n",
            "Downloading https://ultralytics.com/assets/coco128-seg.zip to '/content/datasets/coco128-seg.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.79M/6.79M [00:00<00:00, 84.9MB/s]\n",
            "Unzipping /content/datasets/coco128-seg.zip to /content/datasets/coco128-seg...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 263/263 [00:00<00:00, 3629.14file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success âœ… (0.8s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1150432  ultralytics.nn.modules.head.Segment          [80, 32, 64, [64, 128, 256]]  \n",
            "YOLOv8n-seg summary: 261 layers, 3409968 parameters, 3409952 gradients, 12.8 GFLOPs\n",
            "\n",
            "Transferred 417/417 items from pretrained weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128-seg/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 1485.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco128-seg/labels/train2017.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128-seg/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/segment/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/3      3.42G      1.069      2.569       1.45      1.152        137        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:14<00:00,  1.86s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.581      0.565       0.59      0.443       0.56      0.534      0.552      0.367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/3       3.5G      1.111      2.597      1.404      1.167        180        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.607      0.561      0.607      0.456      0.626      0.522       0.57      0.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/3      3.35G        1.1      2.699      1.363      1.149        291        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:05<00:00,  1.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.651      0.553      0.617       0.46      0.617      0.526      0.581      0.382\n",
            "\n",
            "3 epochs completed in 0.013 hours.\n",
            "Optimizer stripped from runs/segment/train/weights/last.pt, 7.1MB\n",
            "Optimizer stripped from runs/segment/train/weights/best.pt, 7.1MB\n",
            "\n",
            "Validating runs/segment/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-seg summary (fused): 195 layers, 3404320 parameters, 0 gradients, 12.6 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:08<00:00,  2.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        929      0.668      0.541      0.617      0.461      0.644      0.517      0.582      0.383\n",
            "                person        128        254      0.869      0.654      0.784      0.551      0.842      0.627      0.738        0.4\n",
            "               bicycle        128          6      0.563      0.167      0.413      0.233      0.566      0.167      0.362       0.21\n",
            "                   car        128         46      0.563      0.174       0.26       0.15      0.566      0.174      0.232      0.102\n",
            "            motorcycle        128          5          1       0.87      0.995      0.815          1      0.866      0.995      0.591\n",
            "              airplane        128          6      0.667      0.833      0.913      0.767      0.599      0.754      0.885      0.594\n",
            "                   bus        128          7      0.658      0.714      0.721      0.618      0.659      0.714      0.721      0.591\n",
            "                 train        128          3      0.442      0.667       0.83      0.682      0.443      0.667       0.83      0.587\n",
            "                 truck        128         12      0.872       0.25      0.466      0.227      0.883       0.25      0.481      0.223\n",
            "                  boat        128          6      0.561      0.333      0.478      0.296      0.571      0.333      0.391       0.14\n",
            "         traffic light        128         14      0.719      0.214      0.208      0.142      0.481      0.143      0.153      0.143\n",
            "             stop sign        128          2          1      0.867      0.995      0.797          1      0.865      0.995      0.846\n",
            "                 bench        128          9      0.565      0.294      0.551      0.263      0.345      0.182      0.342      0.121\n",
            "                  bird        128         16      0.931      0.846      0.968      0.687      0.931      0.844      0.968      0.526\n",
            "                   cat        128          4      0.727          1      0.995      0.778      0.728          1      0.995      0.846\n",
            "                   dog        128          9      0.597      0.667        0.8       0.63      0.598      0.667      0.741      0.501\n",
            "                 horse        128          2      0.576          1      0.995      0.597      0.289        0.5      0.497      0.199\n",
            "              elephant        128         17      0.952      0.824      0.919      0.716      0.952      0.824      0.881      0.591\n",
            "                  bear        128          1      0.653          1      0.995      0.995      0.655          1      0.995      0.995\n",
            "                 zebra        128          4      0.852          1      0.995      0.971      0.853          1      0.995      0.853\n",
            "               giraffe        128          9      0.879          1      0.995      0.704      0.784      0.889      0.888      0.495\n",
            "              backpack        128          6      0.626      0.333      0.533      0.321      0.632      0.333      0.438      0.289\n",
            "              umbrella        128         18      0.799      0.556      0.734      0.458      0.599      0.416      0.456      0.226\n",
            "               handbag        128         19       0.91      0.105      0.257      0.179      0.917      0.105      0.237      0.125\n",
            "                   tie        128          7      0.789       0.54      0.584      0.412      0.789      0.539      0.584      0.349\n",
            "              suitcase        128          4      0.748       0.75      0.912      0.729      0.756       0.75      0.912      0.658\n",
            "               frisbee        128          5      0.544        0.6      0.663      0.606      0.547        0.6      0.689      0.437\n",
            "                  skis        128          1      0.624          1      0.995      0.796      0.634          1      0.995      0.199\n",
            "             snowboard        128          7      0.562      0.714      0.739      0.558      0.455      0.571      0.494      0.216\n",
            "           sports ball        128          6      0.683      0.367      0.441      0.262      0.364      0.197      0.272      0.209\n",
            "                  kite        128         10      0.683        0.3      0.415        0.2      0.688        0.3      0.402      0.186\n",
            "          baseball bat        128          4          1          0      0.396     0.0878          1          0      0.571      0.222\n",
            "        baseball glove        128          7       0.73      0.429       0.43      0.269      0.732      0.429      0.458      0.315\n",
            "            skateboard        128          5      0.557        0.2      0.377      0.228       0.56        0.2      0.398      0.225\n",
            "         tennis racket        128          7      0.517      0.429      0.413      0.259      0.518      0.429      0.469      0.274\n",
            "                bottle        128         18      0.456      0.326      0.406      0.206       0.43      0.294      0.406      0.207\n",
            "            wine glass        128         16      0.731      0.625      0.701      0.316      0.588        0.5      0.411      0.241\n",
            "                   cup        128         36      0.687      0.305      0.389      0.283      0.684      0.301      0.367      0.259\n",
            "                  fork        128          6      0.435      0.167      0.221      0.193      0.437      0.167      0.216      0.106\n",
            "                 knife        128         16      0.738      0.438      0.557      0.397      0.747      0.438      0.472      0.344\n",
            "                 spoon        128         22      0.807      0.273      0.452      0.299      0.812      0.273      0.445      0.222\n",
            "                  bowl        128         28      0.742      0.617      0.682      0.545      0.696      0.573      0.633      0.353\n",
            "                banana        128          1      0.194          1      0.332      0.298      0.197          1      0.332      0.232\n",
            "              sandwich        128          2      0.342        0.5      0.662      0.596      0.348        0.5      0.662      0.582\n",
            "                orange        128          4          1          0       0.62       0.45          1          0       0.62      0.387\n",
            "              broccoli        128         11      0.403      0.182       0.28       0.23      0.407      0.182      0.316      0.235\n",
            "                carrot        128         24       0.66      0.292      0.576      0.414      0.663      0.292      0.573      0.348\n",
            "               hot dog        128          2      0.396          1      0.828      0.828      0.396          1      0.828      0.713\n",
            "                 pizza        128          5      0.562          1      0.962      0.788      0.564          1      0.962      0.774\n",
            "                 donut        128         14      0.632      0.929      0.948      0.877      0.633      0.929      0.948      0.789\n",
            "                  cake        128          4       0.63          1      0.945      0.881      0.631          1      0.945      0.778\n",
            "                 chair        128         35      0.567      0.457      0.457      0.229      0.606      0.484      0.447      0.205\n",
            "                 couch        128          6      0.547        0.5      0.673      0.545      0.553        0.5      0.527      0.314\n",
            "          potted plant        128         14      0.681      0.459      0.617      0.393       0.68      0.457      0.687      0.306\n",
            "                   bed        128          3          1      0.964      0.995      0.467          1       0.95      0.995       0.47\n",
            "          dining table        128         13      0.567      0.462      0.483      0.383      0.095     0.0769      0.151     0.0746\n",
            "                toilet        128          2      0.518        0.5      0.523      0.518      0.521        0.5      0.523      0.515\n",
            "                    tv        128          2      0.654          1      0.995      0.846      0.661          1      0.995      0.846\n",
            "                laptop        128          3      0.713      0.333      0.495      0.464      0.717      0.333      0.505      0.279\n",
            "                 mouse        128          2          1          0      0.077     0.0194          1          0     0.0235     0.0117\n",
            "                remote        128          8      0.804      0.516      0.625      0.486      0.804      0.514      0.651      0.459\n",
            "            cell phone        128          8      0.649      0.237      0.189      0.105      0.635      0.226      0.181     0.0951\n",
            "             microwave        128          3      0.735      0.939      0.913      0.806      0.732      0.929      0.913      0.773\n",
            "                  oven        128          5      0.474        0.4      0.338      0.248      0.717        0.6      0.607       0.42\n",
            "                  sink        128          6      0.318      0.167      0.257        0.2       0.32      0.167      0.269      0.196\n",
            "          refrigerator        128          5      0.772       0.69      0.765      0.541      0.772      0.686      0.763      0.506\n",
            "                  book        128         29      0.331      0.069      0.346      0.175      0.339      0.069      0.277      0.111\n",
            "                 clock        128          9      0.874      0.771      0.814      0.656      0.874      0.769      0.814      0.677\n",
            "                  vase        128          2       0.33          1      0.497       0.46      0.402          1      0.497       0.41\n",
            "              scissors        128          1          1          0          0          0          1          0          0          0\n",
            "            teddy bear        128         21      0.709      0.233       0.51      0.329      0.708      0.232      0.405      0.222\n",
            "            toothbrush        128          5      0.384        0.4      0.528      0.284      0.389        0.4      0.528      0.266\n",
            "Speed: 0.3ms preprocess, 3.7ms inference, 0.0ms loss, 5.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/train\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 1 bus, 1 skateboard, 49.2ms\n",
            "Speed: 3.3ms preprocess, 49.2ms inference, 9.1ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " masks: ultralytics.engine.results.Masks object\n",
              " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
              " obb: None\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: None\n",
              " save_dir: 'runs/segment/train2'\n",
              " speed: {'preprocess': 3.3037662506103516, 'inference': 49.175262451171875, 'postprocess': 9.063005447387695}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Classification\n",
        "\n",
        "YOLOv8 _classification_ models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n"
      ],
      "metadata": {
        "id": "ax3p94VNK9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
        "model.train(data='mnist160', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "5q9Zu6zlL5rS",
        "outputId": "d6a40aa3-932f-4427-e57c-b53c61a16e28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-cls.pt to 'yolov8n-cls.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.30M/5.30M [00:00<00:00, 70.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=mnist160, epochs=3, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
            "\n",
            "Dataset not found âš ï¸, missing path /content/datasets/mnist160, attempting download...\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/mnist160.zip to '/content/datasets/mnist160.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70.0k/70.0k [00:00<00:00, 35.2MB/s]\n",
            "Unzipping /content/datasets/mnist160.zip to /content/datasets/mnist160...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 9037.12file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success âœ… (1.1s), saved to \u001b[1m/content/datasets/mnist160\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/datasets/mnist160/train... found 80 images in 10 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/datasets/mnist160/test... found 80 images in 10 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n",
            "YOLOv8n-cls summary: 99 layers, 1451098 parameters, 1451098 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/mnist160/train... 80 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 6037.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/mnist160/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/mnist160/test... 80 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 5549.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/mnist160/test.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/3     0.749G      2.334         16        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.95it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        0.1      0.525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/3     0.537G      2.328         16        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.51it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 33.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.075        0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/3     0.537G      2.315         16        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.92it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 31.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all     0.0875      0.625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3 epochs completed in 0.001 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1447690 parameters, 0 gradients, 3.3 GFLOPs\n",
            "WARNING âš ï¸ Dataset 'split=val' not found, using 'split=test' instead.\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/datasets/mnist160/train... found 80 images in 10 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/datasets/mnist160/test... found 80 images in 10 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all     0.0875      0.625\n",
            "Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 224x224 9 0.21, 6 0.12, 0 0.11, 2 0.11, 8 0.11, 4.8ms\n",
            "Speed: 12.9ms preprocess, 4.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: None\n",
              " keypoints: None\n",
              " masks: None\n",
              " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
              " obb: None\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: ultralytics.engine.results.Probs object\n",
              " save_dir: 'runs/classify/train2'\n",
              " speed: {'preprocess': 12.89820671081543, 'inference': 4.834890365600586, 'postprocess': 0.0896453857421875}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pose\n",
        "\n",
        "YOLOv8 _pose_ models use the `-pose` suffix, i.e. `yolov8n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details."
      ],
      "metadata": {
        "id": "SpIaFLiO11TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n pose model\n",
        "model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "si4aKFNg19vX",
        "outputId": "f13b5187-d020-4f58-bd9b-73d9e3196479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.51M/6.51M [00:00<00:00, 84.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=pose, mode=train, model=yolov8n-pose.pt, data=coco8-pose.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/pose/train\n",
            "\n",
            "Dataset 'coco8-pose.yaml' images not found âš ï¸, missing path '/content/datasets/coco8-pose/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8-pose.zip to '/content/datasets/coco8-pose.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334k/334k [00:00<00:00, 10.3MB/s]\n",
            "Unzipping /content/datasets/coco8-pose.zip to /content/datasets/coco8-pose...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 2884.45file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success âœ… (1.4s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1035934  ultralytics.nn.modules.head.Pose             [1, [17, 3], [64, 128, 256]]  \n",
            "YOLOv8n-pose summary: 250 layers, 3295470 parameters, 3295454 gradients, 9.3 GFLOPs\n",
            "\n",
            "Transferred 397/397 items from pretrained weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/pose/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8-pose/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3501.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8-pose/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8-pose/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 998.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8-pose/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/pose/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/pose/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/3     0.803G      1.196      2.997     0.2851     0.8514      1.463         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         14      0.927      0.905      0.907      0.668      0.846        0.5      0.535      0.352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/3     0.791G      1.104      3.305     0.3061     0.8659      1.285         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         14      0.929      0.929      0.907      0.668      0.845        0.5      0.535      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "        3/3     0.786G      1.214      4.142     0.3471      1.108      1.197         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         14      0.903      0.929      0.907      0.668      0.843        0.5      0.535      0.351\n",
            "\n",
            "3 epochs completed in 0.003 hours.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer stripped from runs/pose/train/weights/last.pt, 6.8MB\n",
            "Optimizer stripped from runs/pose/train/weights/best.pt, 6.8MB\n",
            "\n",
            "Validating runs/pose/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-pose summary (fused): 187 layers, 3289964 parameters, 0 gradients, 9.2 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         14      0.927      0.906      0.907      0.668      0.846        0.5      0.535      0.352\n",
            "Speed: 1.3ms preprocess, 5.8ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/pose/train\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 39.5ms\n",
            "Speed: 2.3ms preprocess, 39.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: ultralytics.engine.results.Keypoints object\n",
              " masks: None\n",
              " names: {0: 'person'}\n",
              " obb: None\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: None\n",
              " save_dir: 'runs/pose/train2'\n",
              " speed: {'preprocess': 2.3000240325927734, 'inference': 39.4895076751709, 'postprocess': 2.547740936279297}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Oriented Bounding Boxes (OBB)\n",
        "\n",
        "YOLOv8 _OBB_ models use the `-obb` suffix, i.e. `yolov8n-obb.pt` and are pretrained on the DOTA dataset. See [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for full details."
      ],
      "metadata": {
        "id": "cf5j_T9-B5F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-obb, train it on DOTA8 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-obb.pt')  # load a pretrained YOLOv8n OBB model\n",
        "model.train(data='coco8-dota.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "IJNKClOOB5YS",
        "outputId": "c1e97a9d-4279-46fd-cfb4-950d36a54cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-obb.pt to 'yolov8n-obb.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.24M/6.24M [00:00<00:00, 79.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=obb, mode=train, model=yolov8n-obb.pt, data=coco8-dota.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=1024, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/obb/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Dataset 'coco8-dota.yaml' error âŒ 'coco8-dota.yaml' does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m             }:\n\u001b[0;32m--> 514\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_det_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"yaml_file\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/data/utils.py\u001b[0m in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/checks.py\u001b[0m in \u001b[0;36mcheck_file\u001b[0;34m(file, suffix, download, hard)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{file}' does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: 'coco8-dota.yaml' does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9921ae538d7e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov8n-obb.pt'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load a pretrained YOLOv8n OBB model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coco8-dota.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://ultralytics.com/images/bus.jpg'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# predict on an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# manually set model only if not resuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/obb/train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"obb\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Model and Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_model_file_from_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yaml_file\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# for validating 'yolo train data=url.zip' usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memojis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset 'coco8-dota.yaml' error âŒ 'coco8-dota.yaml' does not exist"
          ]
        }
      ]
    }
  ]
}